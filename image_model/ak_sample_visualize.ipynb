{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/home/muhamhz/jpdvt_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Enable inline plotting for matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.utils import save_image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from einops import rearrange\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "import argparse  # We won't use command-line parsing but keep for reference\n",
    "\n",
    "# Import your custom modules; make sure your PYTHONPATH is set correctly so that these modules are found.\n",
    "from diffusion import create_diffusion\n",
    "from diffusers.models import AutoencoderKL\n",
    "from models import DiT_models, get_2d_sincos_pos_embed\n",
    "from datasets import MET \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow_tensor(img_tensor, title=\"\"):\n",
    "    \"\"\"\n",
    "    Display a tensor as an image.\n",
    "    Assumes the tensor is normalized with mean=0.5 and std=0.5.\n",
    "    \"\"\"\n",
    "    # Unnormalize\n",
    "    img_tensor = img_tensor * 0.5 + 0.5\n",
    "    npimg = img_tensor.permute(1, 2, 0).cpu().numpy()\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.imshow(np.clip(npimg, 0, 1))\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def center_crop_arr(pil_image, image_size):\n",
    "    \"\"\"\n",
    "    Center crop using a method inspired by ADM.\n",
    "    \"\"\"\n",
    "    while min(*pil_image.size) >= 2 * image_size:\n",
    "        pil_image = pil_image.resize(\n",
    "            tuple(x // 2 for x in pil_image.size), resample=Image.BOX\n",
    "        )\n",
    "\n",
    "    scale = image_size / min(*pil_image.size)\n",
    "    pil_image = pil_image.resize(\n",
    "        tuple(round(x * scale) for x in pil_image.size), resample=Image.BICUBIC\n",
    "    )\n",
    "\n",
    "    arr = np.array(pil_image)\n",
    "    crop_y = (arr.shape[0] - image_size) // 2\n",
    "    crop_x = (arr.shape[1] - image_size) // 2\n",
    "    return Image.fromarray(arr[crop_y: crop_y + image_size, crop_x: crop_x + image_size])\n",
    "\n",
    "\n",
    "def find_permutation(distance_matrix):\n",
    "    \"\"\"\n",
    "    Greedy algorithm to find the permutation order based on the distance matrix.\n",
    "    \"\"\"\n",
    "    sort_list = []\n",
    "    for m in range(distance_matrix.shape[1]):\n",
    "        order = distance_matrix[:, 0].argmin()\n",
    "        sort_list.append(order)\n",
    "        distance_matrix = distance_matrix[:, 1:]\n",
    "        distance_matrix[order, :] = 2024\n",
    "    return sort_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    # Set up PyTorch\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.set_grad_enabled(False)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # Create a template for visualization (not used further in this example)\n",
    "    template = np.zeros((6, 6))\n",
    "    for i in range(6):\n",
    "        for j in range(6):\n",
    "            template[i, j] = 18 * i + j\n",
    "    template = np.concatenate((template, template, template), axis=0)\n",
    "    template = np.concatenate((template, template, template), axis=1)\n",
    "    \n",
    "    # Load model:\n",
    "    model = DiT_models[args.model](\n",
    "        input_size=args.image_size,\n",
    "    ).to(device)\n",
    "    print(\"Loading model from:\", args.ckpt)\n",
    "    ckpt_path = args.ckpt \n",
    "    model_dict = model.state_dict()\n",
    "    state_dict = torch.load(ckpt_path, weights_only=False)\n",
    "    model_state_dict = state_dict['model']\n",
    "    pretrained_dict = {k: v for k, v in model_state_dict.items() if k in model_dict}\n",
    "    model.load_state_dict(pretrained_dict, strict=False)\n",
    "\n",
    "    print(\"Model keys (first 10):\", list(model_dict.keys())[:10])\n",
    "    print(\"Checkpoint keys:\", list(model_state_dict.keys()))\n",
    "    \n",
    "    # Define the transformation for the dataset.\n",
    "    transform = transforms.Compose([\n",
    "       transforms.Lambda(lambda pil_image: center_crop_arr(pil_image, 192)),\n",
    "       transforms.ToTensor(),\n",
    "       transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], inplace=True)\n",
    "    ])\n",
    "    \n",
    "    # Set the model to training mode so batchnorm behaves (needed for batch size 1).\n",
    "    model.train() \n",
    "    \n",
    "    # Create the diffusion process.\n",
    "    diffusion = create_diffusion(str(args.num_sampling_steps))\n",
    "    \n",
    "    # Choose dataset based on argument.\n",
    "    if args.dataset == \"met\":\n",
    "        dataset = MET(args.data_path, 'test')\n",
    "    elif args.dataset == \"imagenet\":\n",
    "        dataset = ImageFolder(args.data_path, transform=transform)\n",
    "    \n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=False,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    # Create time embeddings.\n",
    "    time_emb = torch.tensor(get_2d_sincos_pos_embed(8, 3)).unsqueeze(0).float().to(device)\n",
    "    time_emb_noise = torch.tensor(get_2d_sincos_pos_embed(8, 12)).unsqueeze(0).float().to(device)\n",
    "    time_emb_noise = torch.randn_like(time_emb_noise).repeat(1, 1, 1)\n",
    "    model_kwargs = None\n",
    "    \n",
    "    abs_results = []\n",
    "    for x in loader:\n",
    "        # For ImageFolder datasets, unpack the tuple.\n",
    "        if args.dataset == 'imagenet':\n",
    "            x, _ = x\n",
    "        x = x.to(device)\n",
    "        imshow_tensor(x[0], title=\"Original Image\")\n",
    "        \n",
    "        # Optionally crop patches if requested.\n",
    "        if args.dataset == 'imagenet' and args.crop:\n",
    "            centercrop = transforms.CenterCrop((64, 64))\n",
    "            patchs = rearrange(x, 'b c (p1 h1) (p2 w1) -> b c (p1 p2) h1 w1', \n",
    "                               p1=3, p2=3, h1=96, w1=96)\n",
    "            patchs = centercrop(patchs)\n",
    "            x = rearrange(patchs, 'b c (p1 p2) h1 w1 -> b c (p1 h1) (p2 w1)', \n",
    "                          p1=3, p2=3, h1=64, w1=64)\n",
    "        \n",
    "        # Shuffle the patches\n",
    "        indices = np.random.permutation(9)\n",
    "        print(\"Shuffle indices:\", indices)\n",
    "        # Rearrange image into patches\n",
    "        x = rearrange(x, 'b c (p1 h1) (p2 w1) -> b c (p1 p2) h1 w1', \n",
    "                      p1=3, p2=3, h1=args.image_size//3, w1=args.image_size//3)\n",
    "        \n",
    "        # Display patches before permutation\n",
    "        patches = [x[0, :, i, :, :] for i in range(9)]\n",
    "        grid = torch.stack(patches)\n",
    "        save_image(grid, \"debug_patches_before.png\", nrow=3, normalize=True)\n",
    "        plt.figure(figsize=(4, 4))\n",
    "        plt.imshow(torchvision.utils.make_grid(grid, nrow=3, normalize=True).permute(1, 2, 0).cpu().numpy())\n",
    "        plt.title(\"Patches Before Permutation\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "        \n",
    "        # Apply the permutation to the patches\n",
    "        x = x[:, :, indices, :, :]\n",
    "        patches = [x[0, :, i, :, :] for i in range(9)]\n",
    "        grid = torch.stack(patches)\n",
    "        save_image(grid, \"debug_patches_after.png\", nrow=3, normalize=True)\n",
    "        plt.figure(figsize=(4, 4))\n",
    "        plt.imshow(torchvision.utils.make_grid(grid, nrow=3, normalize=True).permute(1, 2, 0).cpu().numpy())\n",
    "        plt.title(\"Patches After Permutation\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "        \n",
    "        # Reconstruct the scrambled image.\n",
    "        x = rearrange(x, 'b c (p1 p2) h1 w1 -> b c (p1 h1) (p2 w1)', \n",
    "                      p1=3, p2=3, h1=args.image_size//3, w1=args.image_size//3)\n",
    "        imshow_tensor(x[0], title=\"Final Scrambled Image\")\n",
    "        print(\"Scrambled image shape:\", x.shape)\n",
    "        \n",
    "        # Generate samples using the diffusion process.\n",
    "        samples = diffusion.p_sample_loop(\n",
    "            model.forward, x, time_emb_noise.shape, time_emb_noise, \n",
    "            clip_denoised=False, model_kwargs=model_kwargs, progress=True, device=device\n",
    "        )\n",
    "        print(\"Generated samples shape:\", samples.shape)\n",
    "        \n",
    "        # Process and compare each sample.\n",
    "        for sample, img in zip(samples, x):\n",
    "            sample = rearrange(sample, '(p1 h1 p2 w1) d -> (p1 p2) (h1 w1) d', \n",
    "                                 p1=3, p2=3, h1=args.image_size//48, w1=args.image_size//48)\n",
    "            sample = sample.mean(1)\n",
    "            dist = pairwise_distances(sample.cpu().numpy(), time_emb[0].cpu().numpy(), metric='manhattan')\n",
    "            order = find_permutation(dist)\n",
    "            pred = np.asarray(order).argsort()\n",
    "            print(\"Predicted order:\", pred)\n",
    "            abs_results.append(int((pred == indices).all()))\n",
    "         \n",
    "        # Report accuracy on this batch.\n",
    "        acc = np.asarray(abs_results).sum() / len(abs_results) if abs_results else 0\n",
    "        print(\"Test result on\", len(abs_results), \"samples:\", acc)\n",
    "        # For demonstration, process only one batch.\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data path: /cluster/home/muhamhz/data/imagenet/val\n",
      "Using checkpoint: /cluster/home/muhamhz/JPDVT/image_model/results/009-imagenet-JPDVT-crop/checkpoints/2850000.pt\n",
      "Loading model from: /cluster/home/muhamhz/JPDVT/image_model/results/009-imagenet-JPDVT-crop/checkpoints/2850000.pt\n"
     ]
    },
    {
     "ename": "UnpicklingError",
     "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL argparse.Namespace was not an allowed global by default. Please use `torch.serialization.add_safe_globals([Namespace])` or the `torch.serialization.safe_globals([Namespace])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing checkpoint:\u001b[39m\u001b[38;5;124m\"\u001b[39m, args\u001b[38;5;241m.\u001b[39mckpt)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Run the main function.\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 22\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     20\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mckpt \n\u001b[1;32m     21\u001b[0m model_dict \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mstate_dict()\n\u001b[0;32m---> 22\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Ensure compatibility with your device\u001b[39;00m\n\u001b[1;32m     23\u001b[0m model_state_dict \u001b[38;5;241m=\u001b[39m state_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     24\u001b[0m pretrained_dict \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m model_state_dict\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m model_dict}\n",
      "File \u001b[0;32m~/jpdvt_env/lib/python3.10/site-packages/torch/serialization.py:1470\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1462\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[1;32m   1463\u001b[0m                     opened_zipfile,\n\u001b[1;32m   1464\u001b[0m                     map_location,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1467\u001b[0m                     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[1;32m   1468\u001b[0m                 )\n\u001b[1;32m   1469\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1470\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1471\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[1;32m   1472\u001b[0m             opened_zipfile,\n\u001b[1;32m   1473\u001b[0m             map_location,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1476\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[1;32m   1477\u001b[0m         )\n\u001b[1;32m   1478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL argparse.Namespace was not an allowed global by default. Please use `torch.serialization.add_safe_globals([Namespace])` or the `torch.serialization.safe_globals([Namespace])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
     ]
    }
   ],
   "source": [
    "# Instead of using argparse, we can set up our arguments directly.\n",
    "from types import SimpleNamespace\n",
    "\n",
    "args = SimpleNamespace(\n",
    "    model=\"JPDVT\",\n",
    "    dataset=\"imagenet\",\n",
    "    data_path=\"val\",  # This is the subfolder inside the base directory.\n",
    "    crop=False,\n",
    "    image_size=192,\n",
    "    num_sampling_steps=250,\n",
    "    seed=0,\n",
    "    ckpt=\"/cluster/home/muhamhz/JPDVT/image_model/results/009-imagenet-JPDVT-crop/checkpoints/2850000.pt\"\n",
    ")\n",
    "\n",
    "# Construct the full data path.\n",
    "base_data_path = \"/cluster/home/muhamhz/data/imagenet/\"\n",
    "args.data_path = os.path.join(base_data_path, args.data_path)\n",
    "print(\"Using data path:\", args.data_path)\n",
    "print(\"Using checkpoint:\", args.ckpt)\n",
    "\n",
    "# Run the main function.\n",
    "main(args)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
